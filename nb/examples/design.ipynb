{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "GALAXY = 'ngc6822'\n",
    "OBS_PATH = '/datascope/subaru/data/cmdfit/dSph'\n",
    "HSC_FILE = '/datascope/subaru/data/cmdfit/dSph/ngc6822_tpall3e_g24.cat'\n",
    "GAIA_FILE = '/datascope/subaru/data/cmdfit/dSph/gaia.h5'\n",
    "SKY_FILE = '/datascope/subaru/data/cmdfit/dSph/sky_ngc6822.feather'\n",
    "FLUXSTD_FILE = '/datascope/subaru/data/cmdfit/dSph/fluxstd_ngc6822.feather'\n",
    "# MLCLASS_FILE = '/datascope/subaru/data/targeting/dSph/umi/ursaminor_mlclass.csv'\n",
    "# PMAP_FILE = '/datascope/subaru/data/cmdfit/run/umi/sim/nobin_chab_nb_250k_001/pmap.h5'\n",
    "\n",
    "# GALAXY = 'umi'\n",
    "# OBS_PATH = '/datascope/subaru/data/cmdfit/dSph'\n",
    "# HSC_FILE = '/datascope/subaru/data/cmdfit/dSph/umi_tpall3e_g24.cat'\n",
    "# GAIA_FILE = '/datascope/subaru/data/cmdfit/dSph/gaia.h5'\n",
    "# SKY_FILE = '/datascope/subaru/data/cmdfit/dSph/sky_ursaminor.feather'\n",
    "# FLUXSTD_FILE = '/datascope/subaru/data/cmdfit/dSph/fluxstd_ursaminor.feather'\n",
    "# MLCLASS_FILE = '/datascope/subaru/data/targeting/dSph/umi/ursaminor_mlclass.csv'\n",
    "# PMAP_FILE = '/datascope/subaru/data/cmdfit/run/umi/sim/nobin_chab_nb_250k_001/pmap.h5'\n",
    "\n",
    "OUTPUT_PATH = '/datascope/subaru/user/dobos/netflow'\n",
    "ISOCHRONES_PATH = '/datascope/subaru/data/cmdfit/isochrones/dartmouth/import/afep0_cfht_sdss_hsc'\n",
    "\n",
    "GAIA_CROSSMATCH_RADIUS = 0.1    # in arcsec\n",
    "\n",
    "NVISITS = 1\n",
    "OUTPUT_PATH = f'/datascope/subaru/user/dobos/netflow/{GALAXY}_{NVISITS}_visit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.special import logsumexp\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from astropy import wcs\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import Angle, SkyCoord\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'debug' not in globals():\n",
    "#     import debugpy\n",
    "#     debugpy.listen(('0.0.0.0', 5698))\n",
    "#     debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=6) #controls default text size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfs.ga.targeting.targets.dsph import *\n",
    "from pfs.ga.targeting.instrument import *\n",
    "from pfs.ga.targeting.diagram import CMD, CCD, FOV, FP, ColorAxis, MagnitudeAxis\n",
    "from pfs.ga.targeting.photometry import Photometry, Magnitude, Color\n",
    "from pfs.ga.targeting.projection import WcsProjection, Pointing\n",
    "from pfs.ga.targeting.netflow import Netflow, Visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy = GALAXIES[GALAXY]\n",
    "hsc = galaxy.get_photometry()\n",
    "cmd = galaxy.get_cmd()\n",
    "ccd = galaxy.get_ccd()\n",
    "gaia_cmd = galaxy.get_cmd(Gaia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointings = galaxy.get_pointings(SubaruPFI)\n",
    "pointing = pointings[0]\n",
    "pointings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs = WcsProjection(pointing, proj='TAN')\n",
    "wfc = SubaruWFC(pointing)\n",
    "fov = FOV(projection=wcs)\n",
    "fp = FP(wfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate pointings and visits\n",
    "\n",
    "This should match the netflow settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointings = []\n",
    "visits = []\n",
    "exp_time = 12 / NVISITS * 900\n",
    "\n",
    "# Observation time is the next day from now at midnight\n",
    "obs_time = datetime.now() + pd.Timedelta(days=1)\n",
    "obs_time = obs_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "obs_time = Time(obs_time)\n",
    "print('obs_time', obs_time)\n",
    "\n",
    "vidx = 0\n",
    "for pidx, p in enumerate((galaxy.get_pointings(SubaruPFI))):\n",
    "    # Need to convert targeting.pointing to netflow.pointing\n",
    "    np = Pointing(\n",
    "        p.ra, p.dec,\n",
    "        posang=p.posang,\n",
    "        # obs_time=Time(datetime.now()),\n",
    "\n",
    "        # Make sure the target is visible at the time of observation!\n",
    "        obs_time=Time('2024-06-10T00:00:00.0', format='isot', scale='utc'),\n",
    "        exp_time = exp_time,\n",
    "        nvisits=NVISITS\n",
    "    )\n",
    "    pointings.append(np)\n",
    "\n",
    "    for i in range(np.nvisits):\n",
    "        nv = Visit(vidx, pidx, np, visit_cost=0)\n",
    "        visits.append(nv)\n",
    "        vidx += 1\n",
    "\n",
    "len(pointings), len(visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfs.ga.targeting.data import Observation, Catalog\n",
    "from pfs.ga.targeting.io import TextObservationReader, DataFrameSerializer\n",
    "from pfs.ga.targeting.util.astro import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = SubaruHSC.text_observation_reader().read(HSC_FILE)\n",
    "# obs.data.shape\n",
    "\n",
    "\n",
    "# This could be done nicer with a FeatherObservationReader or similar\n",
    "obs = Observation()\n",
    "obs.append_photometry(SubaruHSC.photometry())\n",
    "fn = os.path.join(OUTPUT_PATH, f'{GALAXY}_obs.feather')\n",
    "obs._set_data(DataFrameSerializer().read(fn))\n",
    "obs.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mask = obs.data['priority'] > 0\n",
    "obs_mask.shape, obs_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfs.ga.targeting.io import FeatherSkyReader, FeatherFluxStdReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = FeatherSkyReader()\n",
    "sky = r.read(SKY_FILE)\n",
    "sky.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = FeatherFluxStdReader()\n",
    "fluxstd = r.read(FLUXSTD_FILE)\n",
    "fluxstd.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab10')\n",
    "\n",
    "f = plt.figure(figsize=(3, 3), dpi=240)\n",
    "gs = f.add_gridspec(1, 1)\n",
    "\n",
    "ax = f.add_subplot(gs[0], projection=wcs.wcs)\n",
    "# fov.plot_observation(ax, obs, c='lightgray')\n",
    "fov.plot_observation(ax, obs, c='b')\n",
    "fov.plot_observation(ax, sky, c='lightgray')\n",
    "fov.plot_observation(ax, fluxstd, c='r')\n",
    "\n",
    "pfi = SubaruPFI(instrument_options={'layout': 'calibration'})\n",
    "for p in galaxy.get_pointings(SubaruPFI)[:]:\n",
    "    pfi.plot_focal_plane(ax, fov, corners=True, projection=SubaruWFC(p))\n",
    "\n",
    "ax.set_aspect('equal', adjustable='datalim')\n",
    "\n",
    "# f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfs.ga.targeting.io import DataFrameSerializer\n",
    "\n",
    "fn = os.path.join(OUTPUT_PATH, f'{GALAXY}_assignments.feather')\n",
    "\n",
    "s = DataFrameSerializer()\n",
    "assignments = s.read(fn)\n",
    "\n",
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot focal plane and sky coordinates\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(6, 3), dpi=240)\n",
    "\n",
    "axs[0].plot(assignments['fp_x'], assignments['fp_y'], 'o', ms=1, markeredgewidth=0)\n",
    "axs[0].plot(assignments['fp_x'][100], assignments['fp_y'][100], 'or', ms=1, markeredgewidth=0)\n",
    "axs[0].set_aspect('equal', adjustable='datalim')\n",
    "axs[0].set_title(\"Focal plane\")\n",
    "\n",
    "axs[1].plot(assignments['RA'], assignments['Dec'], 'o', ms=1, markeredgewidth=0)\n",
    "axs[1].plot(assignments['RA'][100], assignments['Dec'][100], 'or', ms=1, markeredgewidth=0)\n",
    "# axs[1].set_aspect('equal', adjustable='datalim')\n",
    "axs[1].set_title(\"Sky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate pfsDesign files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of fiber allocations\n",
    "# This should be the number of total visits times the number of fibers the reach the spectrographs 2458\n",
    "assignments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments['pointing_idx'].unique().size, assignments['visit_idx'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in assignments.columns:\n",
    "    print(c, assignments[c].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the fluxes for the observations\n",
    "\n",
    "The design files need the fluxes which are not processed though netflow, so we need to join them here.\n",
    "\n",
    "Fluxes are represented as a list of numbers, one list for each target, along with a list of filter names.\n",
    "\n",
    "Filter names are postfixed with the filter system, e.g. \"r_ps1\".\n",
    "\n",
    "pfsDesign expects three difference fluxes:\n",
    "* PSF flux\n",
    "* Total flux\n",
    "* Fiber flux\n",
    "\n",
    "We are going to store the lists in the DataFrame we pass to `get_pfsDesign`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HSC dSph data files only contain PSF magnitudes, we need to calculate the fluxes\n",
    "# This will create columns like `obs_flux_hsc_g`, 'err_flux_hsc_g', etc.\n",
    "obs.calculate_flux(force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments['targetid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of unique target IDs\n",
    "# Note that `target_id` is not unique in `assignments` because there can be repeated visits.\n",
    "\n",
    "unique_targetid = pd.DataFrame({ 'targetid': assignments['targetid'].unique() })\n",
    "unique_targetid.set_index('targetid', inplace=True)\n",
    "unique_targetid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down observations to only those that are in the assignments\n",
    "assignments_obs = obs.data.set_index('objid').join(unique_targetid, how='inner')\n",
    "\n",
    "# Convert index back to `objid` column\n",
    "assignments_obs.reset_index(inplace=True, names='objid')\n",
    "\n",
    "# Set additional, missing columns\n",
    "assignments_obs['epoch'] = 'J2000.0'                    # Why is it a string? Strings are used for the equinox not epoch\n",
    "assignments_obs['tract'] = 0\n",
    "assignments_obs['patch'] = '0,0'\n",
    "assignments_obs['catid'] = 15001                        # Where do we get these from?\n",
    "assignments_obs['proposalid'] = 'SSP_GA_dSph'\n",
    "\n",
    "assignments_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert flux columns into columns of lists\n",
    "\n",
    "# The design file contains the flux for each target but the list of filters can vary from target to target\n",
    "\n",
    "def flux_to_list(row, prefix='obs'):\n",
    "    return [ row[f'{prefix}_hsc_{b}'] for b in [ 'g', 'i', 'nb515'] ]\n",
    "\n",
    "def filter_to_list(row):\n",
    "    return [ 'hsc_g', 'hsc_i', 'hsc_nb515' ]\n",
    "\n",
    "for prefix in [ 'psf', 'fiber', 'total' ]:\n",
    "    assignments_obs[f'{prefix}_flux'] = assignments_obs.apply(lambda row: flux_to_list(row, 'obs_flux'), axis=1)\n",
    "    assignments_obs[f'{prefix}_flux_err'] = assignments_obs.apply(lambda row: flux_to_list(row, 'err_flux'), axis=1)\n",
    "\n",
    "assignments_obs['filter'] = assignments_obs.apply(filter_to_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_obs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the columns of flux standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxstd.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the flux standards with the assignments to work with a smaller dataset\n",
    "assignments_fluxstd = fluxstd.data.set_index('objid').join(unique_targetid, how='inner')\n",
    "\n",
    "# Convert index back to `objid` column\n",
    "assignments_fluxstd.reset_index(inplace=True, names='objid')\n",
    "\n",
    "# Set additional, missing columns'\n",
    "# assignments_obs['epoch'] = 'J2000.0'  # We get this for the flux standards\n",
    "assignments_fluxstd['tract'] = 0\n",
    "assignments_fluxstd['patch'] = '0,0'\n",
    "assignments_fluxstd['catid'] = -1\n",
    "assignments_fluxstd['proposalid'] = 'N/A'\n",
    "\n",
    "assignments_fluxstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_fluxstd[[ f'filter_{f}' for f in 'grizyj' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert flux columns into columns of lists\n",
    "\n",
    "def flux_to_list(row, prefix='psf_flux'):\n",
    "    fluxes = []\n",
    "    for f in 'grizyj':\n",
    "        if row[f'filter_{f}'] is not None:\n",
    "            fluxes.append(row[f'{prefix}_{f}'])\n",
    "    return fluxes\n",
    "        \n",
    "\n",
    "def filter_to_list(row):\n",
    "    filters = []\n",
    "    for f in 'grizyj':\n",
    "        if row[f'filter_{f}'] is not None:\n",
    "            filters.append(row[f'filter_{f}'])\n",
    "    return filters\n",
    "\n",
    "for prefix in [ 'psf', 'fiber', 'total' ]:\n",
    "    assignments_fluxstd[f'{prefix}_flux'] = assignments_fluxstd.apply(lambda row: flux_to_list(row, 'psf_flux'), axis=1)\n",
    "    assignments_fluxstd[f'{prefix}_flux_err'] = assignments_fluxstd.apply(lambda row: flux_to_list(row, 'psf_flux_error'), axis=1)\n",
    "\n",
    "assignments_fluxstd['filter'] = assignments_fluxstd.apply(filter_to_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_fluxstd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_fluxstd[['filter', 'psf_flux', 'psf_flux_err', 'total_flux', 'total_flux_err']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick only columns required for PfsDesign\n",
    "columns = [ 'objid', 'epoch', 'proposalid', 'tract', 'patch', 'catid',\n",
    "           'filter', 'psf_flux', 'psf_flux_err', 'fiber_flux', 'fiber_flux_err', 'total_flux', 'total_flux_err' ]\n",
    "\n",
    "assignments_all = pd.concat([ assignments_obs[columns], assignments_fluxstd[columns] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in assignments_all.columns:\n",
    "    print(c, assignments_all[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the assignments with the fluxes etc. calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfs.ga.targeting.netflow.util import *\n",
    "from pfs.datamodel import TargetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignments.join(obs.data[cols], on=['targetid', 'objid'], how='left')\n",
    "\n",
    "a = assignments.set_index('targetid')\n",
    "b = assignments_all.set_index('objid')\n",
    "\n",
    "# Only include columns that are not in a\n",
    "cols = b.columns.difference(a.columns)\n",
    "print(cols)\n",
    "\n",
    "all_assignments = pd_to_nullable(a).join(pd_to_nullable(b[cols]), how='left')\n",
    "all_assignments.reset_index(inplace=True, names='targetid')\n",
    "\n",
    "for c in all_assignments.columns:\n",
    "    print(c, all_assignments[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the catid column to -1 for sky fibers and unassigned fibers\n",
    "# This is necessary to be able to validate the design\n",
    "all_assignments.loc[all_assignments['target_type'] == TargetType.SKY, 'catid'] = -1\n",
    "all_assignments.loc[all_assignments['target_type'] == TargetType.UNASSIGNED, 'catid'] = -1\n",
    "all_assignments.loc[all_assignments['target_type'] == TargetType.UNASSIGNED, 'prefix'] = 'ua'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assignments[all_assignments['prefix'] == 'sci'][['targetid', 'catid', 'target_type', 'fiber_status', 'filter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assignments[all_assignments['prefix'] == 'sky'][['targetid', 'catid', 'target_type', 'fiber_status', 'filter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assignments[all_assignments['prefix'] == 'cal'][['targetid', 'catid', 'target_type', 'fiber_status', 'filter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assignments['prefix'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in all_assignments.columns:\n",
    "    print(c, all_assignments[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some final polishing\n",
    "\n",
    "# TODO: determine tract and patch from the coordinates\n",
    "all_assignments['tract'] = 0\n",
    "all_assignments['patch'] = '0,0'\n",
    "all_assignments['obcode'] = '-1'\n",
    "\n",
    "# Replace None with empty lists in columns containing lists\n",
    "all_assignments['filter'] = all_assignments['filter'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "for prefix in ['fiber', 'psf', 'total']:\n",
    "    all_assignments[f'{prefix}_flux'] = all_assignments[f'{prefix}_flux'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    all_assignments[f'{prefix}_flux_err'] = all_assignments[f'{prefix}_flux_err'].apply(lambda x: x if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (all_assignments['visit_idx'] == 0) & (all_assignments['targetid'] != -1)\n",
    "\n",
    "# Count the number of target_id occurances in all_assignments\n",
    "all_assignments['targetid'][mask].value_counts()\n",
    "\n",
    "# all_assignments[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfs.ga.targeting.netflow import Design\n",
    "\n",
    "designs = []\n",
    "for visit in visits:\n",
    "    d = Design.create_pfsDesign_visit(visit, all_assignments)\n",
    "    d.designName = f'ga_{galaxy.ID}'\n",
    "    designs.append(d)\n",
    "\n",
    "    print(hex(d.pfsDesignId), d.designName, d.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to FITS\n",
    "for d in designs:\n",
    "    d.write(dirName=OUTPUT_PATH)\n",
    "    print(d.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read back the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "# Load back the design file with astropy for direct inspection\n",
    "fn\n",
    "with fits.open(os.path.join(OUTPUT_PATH, designs[0].filename), memmap=False) as hdul:\n",
    "    hdul.info()\n",
    "    # print(hdul[0].header)\n",
    "    # print(hdul[1].header)\n",
    "    # print(hdul[2].header)\n",
    "\n",
    "    data = Table(hdul[1].data)\n",
    "    \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dobos-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}